{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nthieu211\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import emoji\n",
    "import underthesea\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "PRETRAINED_MODEL = 'vinai/phobert-base-v2'\n",
    "DROPOUT = 0.4\n",
    "MAX_LEN = 120\n",
    "\n",
    "API_KEY = 'AIzaSyC-klgvTp1PFc7XMAJKypphlJnOqAIUJn4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Download PhoBERT model\n",
    "phobert = AutoModel.from_pretrained(PRETRAINED_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Remove HTML code\n",
    "def remove_HTML(text):\n",
    "    return re.sub(r'<[^>]*>', '', text)\n",
    "\n",
    "# Standardize unicode\n",
    "def convert_unicode(text):\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "    charutf8 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "    char1252 = char1252.split('|')\n",
    "    charutf8 = charutf8.split('|')\n",
    "\n",
    "    dic = {}\n",
    "    for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dic[x.group()], text\n",
    "    )\n",
    "    \n",
    "# Standardize accent typing\n",
    "vowels_to_ids = {}\n",
    "vowels_table = [\n",
    "    ['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a' ],\n",
    "    ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "    ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "    ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e' ],\n",
    "    ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "    ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i' ],\n",
    "    ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o' ],\n",
    "    ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "    ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "    ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u' ],\n",
    "    ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "    ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y' ]\n",
    "]\n",
    "\n",
    "for i in range(len(vowels_table)):\n",
    "    for j in range(len(vowels_table[i]) - 1):\n",
    "        vowels_to_ids[vowels_table[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def is_valid_vietnamese_word(word):\n",
    "    chars = list(word)\n",
    "    vowel_indexes = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if vowel_indexes == -1: vowel_indexes = index\n",
    "            else:\n",
    "                if index - vowel_indexes != 1: return False\n",
    "                vowel_indexes = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def standardize_word_typing(word):\n",
    "    if not is_valid_vietnamese_word(word): return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    vowel_indexes = []\n",
    "    qu_or_gi = False\n",
    "\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = vowels_table[x][0]\n",
    "\n",
    "        if not qu_or_gi or index != 1:\n",
    "            vowel_indexes.append(index)\n",
    "\n",
    "    if len(vowel_indexes) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = vowels_to_ids.get(chars[1])\n",
    "                chars[1] = vowels_table[x][dau_cau]\n",
    "            else:\n",
    "                x, y = vowels_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1: chars[2] = vowels_table[x][dau_cau]\n",
    "                else: chars[1] = vowels_table[5][dau_cau] if chars[1] == 'i' else vowels_table[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in vowel_indexes:\n",
    "        x, y = vowels_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = vowels_table[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(vowel_indexes) == 2:\n",
    "        if vowel_indexes[-1] == len(chars) - 1:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[0]]]\n",
    "            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]\n",
    "        else:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    else:\n",
    "        x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def standardize_sentence_typing(text):\n",
    "    words = text.lower().split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        if len(cw) == 3: cw[1] = standardize_word_typing(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Normalize acronyms\n",
    "!rm -rf teencode.txt\n",
    "!wget https://gist.githubusercontent.com/nguyenvanhieuvn/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt\n",
    "\n",
    "replace_list = {\n",
    "    'ô kêi': 'ok', 'okie': 'ok', 'o kê': 'ok', 'okey': 'ok', 'ôkê': 'ok', 'ô kê': 'ok', 'oki': 'ok', 'oke': 'ok', 'okay': 'ok', 'okê': 'ok',\n",
    "    'tks': 'cảm ơn', 'thks': 'cảm ơn', 'thanks': 'cảm ơn', 'ths': 'cảm ơn', 'thank': 'cảm ơn',\n",
    "    'kg': 'không', 'not': 'không', 'k': 'không', 'kh': 'không', 'kô': 'không', 'hok': 'không', 'ko': 'không', 'khong': 'không', 'kp': 'không phải',\n",
    "    'he he': 'cười', 'hehe': 'cười', 'hihi': 'cười', 'haha': 'cười', 'hjhj': 'cười', 'thick': 'thích',\n",
    "    'cc': 'chửi tục', 'huhu': 'khóc', 'cute': 'dễ thương', 'cặc':'chửi tục', 'dm':'chửi tục', 'dmm':'chửi tục', 'dume':'chửi tục',\n",
    "\n",
    "    'sz': 'cỡ', 'size': 'cỡ',\n",
    "    'wa': 'quá', 'wá': 'quá', 'qá': 'quá',\n",
    "    'đx': 'được', 'dk': 'được', 'dc': 'được', 'đk': 'được', 'đc': 'được',\n",
    "    'vs': 'với', 'j': 'gì', '“': ' ', 'time': 'thời gian', 'm': 'mình', 'mik': 'mình', 'r': 'rồi', 'bjo': 'bao giờ', 'very': 'rất',\n",
    "\n",
    "    'authentic': 'chuẩn chính hãng', 'aut': 'chuẩn chính hãng', 'auth': 'chuẩn chính hãng', 'date': 'hạn sử dụng', 'hsd': 'hạn sử dụng',\n",
    "    'store': 'cửa hàng', 'sop': 'cửa hàng', 'sốp': 'cửa hàng', 'shop': 'cửa hàng',\n",
    "    'sp': 'sản phẩm', 'product': 'sản phẩm', 'hàg': 'hàng',\n",
    "    'sgk':'sách giáo khoa', 'cv':'công việc',\n",
    "    'ship': 'giao hàng', 'delivery': 'giao hàng', 'síp': 'giao hàng', 'order': 'đặt hàng',\n",
    "\n",
    "    'gud': 'tốt', 'wel done': 'tốt', 'good': 'tốt', 'gút': 'tốt', 'tot': 'tốt', 'nice': 'tốt', 'perfect': 'rất tốt',\n",
    "    'quality': 'chất lượng', 'chất lg': 'chất lượng', 'excelent': 'hoàn hảo', 'bt': 'bình thường', 'bth': 'bình thường',\n",
    "    'sad': 'tệ', 'por': 'tệ', 'poor': 'tệ', 'bad': 'tệ',\n",
    "    'beautiful': 'đẹp tuyệt vời', 'dep': 'đẹp',\n",
    "    'xau': 'xấu', 'sấu': 'xấu',\n",
    "\n",
    "    'thik': 'thích', 'iu': 'yêu', 'fake': 'giả mạo',\n",
    "    'quickly': 'nhanh', 'quick': 'nhanh', 'fast': 'nhanh',\n",
    "    'fresh': 'tươi', 'delicious': 'ngon',\n",
    "\n",
    "    'dt': 'điện thoại', 'fb': 'facebook', 'face': 'facebook', 'ks': 'khách sạn', 'nv': 'nhân viên',\n",
    "    'nt': 'nhắn tin', 'ib': 'nhắn tin', 'tl': 'trả lời', 'trl': 'trả lời', 'rep': 'trả lời',\n",
    "    'fback': 'feedback', 'fedback': 'feedback',\n",
    "    'sd': 'sử dụng', 'sài': 'xài',\n",
    "\n",
    "    '^_^': 'cười', ':)': 'mỉm cười', ':(': 'buồn', '=))': 'cười',\n",
    "    '❤️': 'yêu thích', '👍': 'thích', '🎉': 'chúc mừng', '😀': 'cười', '😍': 'yêu thích', '😂': 'cười chảy nước mắt', '🤗': 'vỗ tay', '😙': 'cười', '🙂': 'mỉm cười',\n",
    "    '😔': 'buồn', '😓': 'buồn', 'T_T': 'khóc', '😭': 'khóc lớn',\n",
    "    '⭐': 'star', '*': 'star', '🌟': 'star',\n",
    "}\n",
    "\n",
    "with open('teencode.txt', encoding='utf-8') as f:\n",
    "    for pair in f.readlines():\n",
    "        key, value = pair.split('\\t')\n",
    "        replace_list[key] = value.strip()\n",
    "\n",
    "\n",
    "def normalize_acronyms(text):\n",
    "    words = []\n",
    "    for word in text.strip().split():\n",
    "        if word.lower() not in replace_list.keys(): words.append(word)\n",
    "        else: words.append(replace_list[word.lower()])\n",
    "    return emoji.demojize(' '.join(words)) # Remove Emojis\n",
    "\n",
    "# Remove unnecessary characters\n",
    "def remove_unnecessary_characters(text):\n",
    "    text = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬÉÈẺẼẸÊẾỀỂỄỆÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÍÌỈĨỊÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴĐ_]', ' ', text)\n",
    "    text = re.sub(r\"[\\.,\\?]+$-\", \"\", text)\n",
    "    text = text.replace(\",\", \" \").replace(\".\", \" \") \\\n",
    "        .replace(\";\", \" \").replace(\"“\", \" \") \\\n",
    "        .replace(\":\", \" \").replace(\"”\", \" \") \\\n",
    "        .replace('\"', \" \").replace(\"'\", \" \") \\\n",
    "        .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
    "        .replace(\"-\", \" \").replace(\"?\", \" \")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Word segmentation & Tokenize\n",
    "def word_segmentation(line):\n",
    "    return underthesea.word_tokenize(line, format=\"text\")\n",
    "\n",
    "def text_preprocess(text):\n",
    "    text = remove_HTML(text)\n",
    "    text = convert_unicode(text)\n",
    "    text = standardize_sentence_typing(text)\n",
    "    text = normalize_acronyms(text)\n",
    "    text = word_segmentation(text) # required for PhoBERT\n",
    "    text = remove_unnecessary_characters(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = phobert\n",
    "        self.drop = nn.Dropout(p=DROPOUT)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_hidden_state, output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False # Dropout will errors if without this\n",
    "        )\n",
    "\n",
    "        x = self.drop(output)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f'weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Enjoyment', 'Disgust', 'Sadness', 'Anger', 'Surprise', 'Fear', 'Other']\n",
    "\n",
    "def infer(text, tokenizer, max_len=MAX_LEN):\n",
    "    clean_text = text_preprocess(text)\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "        clean_text,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, y_pred = torch.max(output, dim=1)\n",
    "\n",
    "    # print(f'Clean text: {clean_text}')\n",
    "    # print(f'Sentiment: {class_names[y_pred]}')\n",
    "    return class_names[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_comments(video_id, api_key):\n",
    "    comments_list = []\n",
    "    # creating youtube resource object\n",
    "    youtube = build('youtube', 'v3',\n",
    "                    developerKey=api_key)\n",
    "    # retrieve youtube video results\n",
    "    video_response=youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId=video_id\n",
    "    ).execute()\n",
    " \n",
    "    # iterate video response\n",
    "    while video_response:\n",
    "        # extracting required info\n",
    "        # from each result object\n",
    "        for item in video_response['items']:\n",
    "            # Extracting comments\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            author =  item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "            # Append comment and author as a dictionary to the list\n",
    "            comments_list.append({'Author': author, 'Comment': comment})\n",
    "\n",
    "        # Again repeat\n",
    "        if 'nextPageToken' in video_response:\n",
    "            video_response = youtube.commentThreads().list(\n",
    "                    part = 'snippet',\n",
    "                    videoId = video_id,\n",
    "                      pageToken = video_response['nextPageToken']\n",
    "                ).execute()\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return pd.DataFrame(comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_ID = \"_ytf121lEbs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = video_comments(VIDEO_ID, API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through comments and predict emotions\n",
    "for index, row in comments_df.iterrows():\n",
    "    comment = row['Comment']\n",
    "    comments_df.at[index, 'Emotion'] = str(infer(comment,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mi kieu</td>\n",
       "      <td>chúc ah độ nhiều sức khoẻ làm live đến 100 tuổi</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aemi</td>\n",
       "      <td>a masew như gia cát lượng í nhờ, làm được beat...</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HTL QC</td>\n",
       "      <td>D2T tưởng về đội của Big rồi thì đi theo team ...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trongduc9</td>\n",
       "      <td>thôi cũng muộn rồi các e về mẹ đi nhể</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Myanhh</td>\n",
       "      <td>tuti hiền ác</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Long Nguyễn</td>\n",
       "      <td>hí hí em xem đầu ạ</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Công Chí</td>\n",
       "      <td>Xem đầu haha</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Mai Phương</td>\n",
       "      <td>Đầu</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Linh xinh ngai</td>\n",
       "      <td>Hello a</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>upclx123</td>\n",
       "      <td>Ác =)))</td>\n",
       "      <td>Disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author                                            Comment  \\\n",
       "0           mi kieu    chúc ah độ nhiều sức khoẻ làm live đến 100 tuổi   \n",
       "1              aemi  a masew như gia cát lượng í nhờ, làm được beat...   \n",
       "2            HTL QC  D2T tưởng về đội của Big rồi thì đi theo team ...   \n",
       "3         trongduc9              thôi cũng muộn rồi các e về mẹ đi nhể   \n",
       "4            Myanhh                                       tuti hiền ác   \n",
       "..              ...                                                ...   \n",
       "306     Long Nguyễn                                 hí hí em xem đầu ạ   \n",
       "307        Công Chí                                       Xem đầu haha   \n",
       "308      Mai Phương                                                Đầu   \n",
       "309  Linh xinh ngai                                            Hello a   \n",
       "310        upclx123                                            Ác =)))   \n",
       "\n",
       "       Emotion  \n",
       "0    Enjoyment  \n",
       "1    Enjoyment  \n",
       "2        Other  \n",
       "3        Other  \n",
       "4    Enjoyment  \n",
       "..         ...  \n",
       "306  Enjoyment  \n",
       "307  Enjoyment  \n",
       "308      Other  \n",
       "309  Enjoyment  \n",
       "310    Disgust  \n",
       "\n",
       "[311 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.to_excel('Comments.xlsx', index=False)\n",
    "comments_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_sent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
