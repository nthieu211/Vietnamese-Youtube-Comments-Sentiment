{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nthieu211\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import emoji\n",
    "import underthesea\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "PRETRAINED_MODEL = 'vinai/phobert-base-v2'\n",
    "DROPOUT = 0.4\n",
    "MAX_LEN = 120\n",
    "\n",
    "API_KEY = 'AIzaSyC-klgvTp1PFc7XMAJKypphlJnOqAIUJn4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Download PhoBERT model\n",
    "phobert = AutoModel.from_pretrained(PRETRAINED_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Remove HTML code\n",
    "def remove_HTML(text):\n",
    "    return re.sub(r'<[^>]*>', '', text)\n",
    "\n",
    "# Standardize unicode\n",
    "def convert_unicode(text):\n",
    "    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'\n",
    "    charutf8 = 'Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´'\n",
    "    char1252 = char1252.split('|')\n",
    "    charutf8 = charutf8.split('|')\n",
    "\n",
    "    dic = {}\n",
    "    for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]\n",
    "    return re.sub(\n",
    "        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',\n",
    "        lambda x: dic[x.group()], text\n",
    "    )\n",
    "    \n",
    "# Standardize accent typing\n",
    "vowels_to_ids = {}\n",
    "vowels_table = [\n",
    "    ['a', 'Ã ', 'Ã¡', 'áº£', 'Ã£', 'áº¡', 'a' ],\n",
    "    ['Äƒ', 'áº±', 'áº¯', 'áº³', 'áºµ', 'áº·', 'aw'],\n",
    "    ['Ã¢', 'áº§', 'áº¥', 'áº©', 'áº«', 'áº­', 'aa'],\n",
    "    ['e', 'Ã¨', 'Ã©', 'áº»', 'áº½', 'áº¹', 'e' ],\n",
    "    ['Ãª', 'á»', 'áº¿', 'á»ƒ', 'á»…', 'á»‡', 'ee'],\n",
    "    ['i', 'Ã¬', 'Ã­', 'á»‰', 'Ä©', 'á»‹', 'i' ],\n",
    "    ['o', 'Ã²', 'Ã³', 'á»', 'Ãµ', 'á»', 'o' ],\n",
    "    ['Ã´', 'á»“', 'á»‘', 'á»•', 'á»—', 'á»™', 'oo'],\n",
    "    ['Æ¡', 'á»', 'á»›', 'á»Ÿ', 'á»¡', 'á»£', 'ow'],\n",
    "    ['u', 'Ã¹', 'Ãº', 'á»§', 'Å©', 'á»¥', 'u' ],\n",
    "    ['Æ°', 'á»«', 'á»©', 'á»­', 'á»¯', 'á»±', 'uw'],\n",
    "    ['y', 'á»³', 'Ã½', 'á»·', 'á»¹', 'á»µ', 'y' ]\n",
    "]\n",
    "\n",
    "for i in range(len(vowels_table)):\n",
    "    for j in range(len(vowels_table[i]) - 1):\n",
    "        vowels_to_ids[vowels_table[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def is_valid_vietnamese_word(word):\n",
    "    chars = list(word)\n",
    "    vowel_indexes = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if vowel_indexes == -1: vowel_indexes = index\n",
    "            else:\n",
    "                if index - vowel_indexes != 1: return False\n",
    "                vowel_indexes = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def standardize_word_typing(word):\n",
    "    if not is_valid_vietnamese_word(word): return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    vowel_indexes = []\n",
    "    qu_or_gi = False\n",
    "\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = vowels_table[x][0]\n",
    "\n",
    "        if not qu_or_gi or index != 1:\n",
    "            vowel_indexes.append(index)\n",
    "\n",
    "    if len(vowel_indexes) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = vowels_to_ids.get(chars[1])\n",
    "                chars[1] = vowels_table[x][dau_cau]\n",
    "            else:\n",
    "                x, y = vowels_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1: chars[2] = vowels_table[x][dau_cau]\n",
    "                else: chars[1] = vowels_table[5][dau_cau] if chars[1] == 'i' else vowels_table[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in vowel_indexes:\n",
    "        x, y = vowels_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # Ãª, Æ¡\n",
    "            chars[index] = vowels_table[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(vowel_indexes) == 2:\n",
    "        if vowel_indexes[-1] == len(chars) - 1:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[0]]]\n",
    "            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]\n",
    "        else:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    else:\n",
    "        x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def standardize_sentence_typing(text):\n",
    "    words = text.lower().split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        if len(cw) == 3: cw[1] = standardize_word_typing(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Normalize acronyms\n",
    "!rm -rf teencode.txt\n",
    "!wget https://gist.githubusercontent.com/nguyenvanhieuvn/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt\n",
    "\n",
    "replace_list = {\n",
    "    'Ã´ kÃªi': 'ok', 'okie': 'ok', 'o kÃª': 'ok', 'okey': 'ok', 'Ã´kÃª': 'ok', 'Ã´ kÃª': 'ok', 'oki': 'ok', 'oke': 'ok', 'okay': 'ok', 'okÃª': 'ok',\n",
    "    'tks': 'cáº£m Æ¡n', 'thks': 'cáº£m Æ¡n', 'thanks': 'cáº£m Æ¡n', 'ths': 'cáº£m Æ¡n', 'thank': 'cáº£m Æ¡n',\n",
    "    'kg': 'khÃ´ng', 'not': 'khÃ´ng', 'k': 'khÃ´ng', 'kh': 'khÃ´ng', 'kÃ´': 'khÃ´ng', 'hok': 'khÃ´ng', 'ko': 'khÃ´ng', 'khong': 'khÃ´ng', 'kp': 'khÃ´ng pháº£i',\n",
    "    'he he': 'cÆ°á»i', 'hehe': 'cÆ°á»i', 'hihi': 'cÆ°á»i', 'haha': 'cÆ°á»i', 'hjhj': 'cÆ°á»i', 'thick': 'thÃ­ch',\n",
    "    'cc': 'chá»­i tá»¥c', 'huhu': 'khÃ³c', 'cute': 'dá»… thÆ°Æ¡ng', 'cáº·c':'chá»­i tá»¥c', 'dm':'chá»­i tá»¥c', 'dmm':'chá»­i tá»¥c', 'dume':'chá»­i tá»¥c',\n",
    "\n",
    "    'sz': 'cá»¡', 'size': 'cá»¡',\n",
    "    'wa': 'quÃ¡', 'wÃ¡': 'quÃ¡', 'qÃ¡': 'quÃ¡',\n",
    "    'Ä‘x': 'Ä‘Æ°á»£c', 'dk': 'Ä‘Æ°á»£c', 'dc': 'Ä‘Æ°á»£c', 'Ä‘k': 'Ä‘Æ°á»£c', 'Ä‘c': 'Ä‘Æ°á»£c',\n",
    "    'vs': 'vá»›i', 'j': 'gÃ¬', 'â€œ': ' ', 'time': 'thá»i gian', 'm': 'mÃ¬nh', 'mik': 'mÃ¬nh', 'r': 'rá»“i', 'bjo': 'bao giá»', 'very': 'ráº¥t',\n",
    "\n",
    "    'authentic': 'chuáº©n chÃ­nh hÃ£ng', 'aut': 'chuáº©n chÃ­nh hÃ£ng', 'auth': 'chuáº©n chÃ­nh hÃ£ng', 'date': 'háº¡n sá»­ dá»¥ng', 'hsd': 'háº¡n sá»­ dá»¥ng',\n",
    "    'store': 'cá»­a hÃ ng', 'sop': 'cá»­a hÃ ng', 'sá»‘p': 'cá»­a hÃ ng', 'shop': 'cá»­a hÃ ng',\n",
    "    'sp': 'sáº£n pháº©m', 'product': 'sáº£n pháº©m', 'hÃ g': 'hÃ ng',\n",
    "    'sgk':'sÃ¡ch giÃ¡o khoa', 'cv':'cÃ´ng viá»‡c',\n",
    "    'ship': 'giao hÃ ng', 'delivery': 'giao hÃ ng', 'sÃ­p': 'giao hÃ ng', 'order': 'Ä‘áº·t hÃ ng',\n",
    "\n",
    "    'gud': 'tá»‘t', 'wel done': 'tá»‘t', 'good': 'tá»‘t', 'gÃºt': 'tá»‘t', 'tot': 'tá»‘t', 'nice': 'tá»‘t', 'perfect': 'ráº¥t tá»‘t',\n",
    "    'quality': 'cháº¥t lÆ°á»£ng', 'cháº¥t lg': 'cháº¥t lÆ°á»£ng', 'excelent': 'hoÃ n háº£o', 'bt': 'bÃ¬nh thÆ°á»ng', 'bth': 'bÃ¬nh thÆ°á»ng',\n",
    "    'sad': 'tá»‡', 'por': 'tá»‡', 'poor': 'tá»‡', 'bad': 'tá»‡',\n",
    "    'beautiful': 'Ä‘áº¹p tuyá»‡t vá»i', 'dep': 'Ä‘áº¹p',\n",
    "    'xau': 'xáº¥u', 'sáº¥u': 'xáº¥u',\n",
    "\n",
    "    'thik': 'thÃ­ch', 'iu': 'yÃªu', 'fake': 'giáº£ máº¡o',\n",
    "    'quickly': 'nhanh', 'quick': 'nhanh', 'fast': 'nhanh',\n",
    "    'fresh': 'tÆ°Æ¡i', 'delicious': 'ngon',\n",
    "\n",
    "    'dt': 'Ä‘iá»‡n thoáº¡i', 'fb': 'facebook', 'face': 'facebook', 'ks': 'khÃ¡ch sáº¡n', 'nv': 'nhÃ¢n viÃªn',\n",
    "    'nt': 'nháº¯n tin', 'ib': 'nháº¯n tin', 'tl': 'tráº£ lá»i', 'trl': 'tráº£ lá»i', 'rep': 'tráº£ lá»i',\n",
    "    'fback': 'feedback', 'fedback': 'feedback',\n",
    "    'sd': 'sá»­ dá»¥ng', 'sÃ i': 'xÃ i',\n",
    "\n",
    "    '^_^': 'cÆ°á»i', ':)': 'má»‰m cÆ°á»i', ':(': 'buá»“n', '=))': 'cÆ°á»i',\n",
    "    'â¤ï¸': 'yÃªu thÃ­ch', 'ğŸ‘': 'thÃ­ch', 'ğŸ‰': 'chÃºc má»«ng', 'ğŸ˜€': 'cÆ°á»i', 'ğŸ˜': 'yÃªu thÃ­ch', 'ğŸ˜‚': 'cÆ°á»i cháº£y nÆ°á»›c máº¯t', 'ğŸ¤—': 'vá»— tay', 'ğŸ˜™': 'cÆ°á»i', 'ğŸ™‚': 'má»‰m cÆ°á»i',\n",
    "    'ğŸ˜”': 'buá»“n', 'ğŸ˜“': 'buá»“n', 'T_T': 'khÃ³c', 'ğŸ˜­': 'khÃ³c lá»›n',\n",
    "    'â­': 'star', '*': 'star', 'ğŸŒŸ': 'star',\n",
    "}\n",
    "\n",
    "with open('teencode.txt', encoding='utf-8') as f:\n",
    "    for pair in f.readlines():\n",
    "        key, value = pair.split('\\t')\n",
    "        replace_list[key] = value.strip()\n",
    "\n",
    "\n",
    "def normalize_acronyms(text):\n",
    "    words = []\n",
    "    for word in text.strip().split():\n",
    "        if word.lower() not in replace_list.keys(): words.append(word)\n",
    "        else: words.append(replace_list[word.lower()])\n",
    "    return emoji.demojize(' '.join(words)) # Remove Emojis\n",
    "\n",
    "# Remove unnecessary characters\n",
    "def remove_unnecessary_characters(text):\n",
    "    text = re.sub(r'[^\\s\\wÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘ÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†Ã“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢ÃÃŒá»ˆÄ¨á»ŠÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä_]', ' ', text)\n",
    "    text = re.sub(r\"[\\.,\\?]+$-\", \"\", text)\n",
    "    text = text.replace(\",\", \" \").replace(\".\", \" \") \\\n",
    "        .replace(\";\", \" \").replace(\"â€œ\", \" \") \\\n",
    "        .replace(\":\", \" \").replace(\"â€\", \" \") \\\n",
    "        .replace('\"', \" \").replace(\"'\", \" \") \\\n",
    "        .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
    "        .replace(\"-\", \" \").replace(\"?\", \" \")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Word segmentation & Tokenize\n",
    "def word_segmentation(line):\n",
    "    return underthesea.word_tokenize(line, format=\"text\")\n",
    "\n",
    "def text_preprocess(text):\n",
    "    text = remove_HTML(text)\n",
    "    text = convert_unicode(text)\n",
    "    text = standardize_sentence_typing(text)\n",
    "    text = normalize_acronyms(text)\n",
    "    text = word_segmentation(text) # required for PhoBERT\n",
    "    text = remove_unnecessary_characters(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = phobert\n",
    "        self.drop = nn.Dropout(p=DROPOUT)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_hidden_state, output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False # Dropout will errors if without this\n",
    "        )\n",
    "\n",
    "        x = self.drop(output)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f'weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Enjoyment', 'Disgust', 'Sadness', 'Anger', 'Surprise', 'Fear', 'Other']\n",
    "\n",
    "def infer(text, tokenizer, max_len=MAX_LEN):\n",
    "    clean_text = text_preprocess(text)\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "        clean_text,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, y_pred = torch.max(output, dim=1)\n",
    "\n",
    "    # print(f'Clean text: {clean_text}')\n",
    "    # print(f'Sentiment: {class_names[y_pred]}')\n",
    "    return class_names[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_comments(video_id, api_key):\n",
    "    comments_list = []\n",
    "    # creating youtube resource object\n",
    "    youtube = build('youtube', 'v3',\n",
    "                    developerKey=api_key)\n",
    "    # retrieve youtube video results\n",
    "    video_response=youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId=video_id\n",
    "    ).execute()\n",
    " \n",
    "    # iterate video response\n",
    "    while video_response:\n",
    "        # extracting required info\n",
    "        # from each result object\n",
    "        for item in video_response['items']:\n",
    "            # Extracting comments\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            author =  item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "            # Append comment and author as a dictionary to the list\n",
    "            comments_list.append({'Author': author, 'Comment': comment})\n",
    "\n",
    "        # Again repeat\n",
    "        if 'nextPageToken' in video_response:\n",
    "            video_response = youtube.commentThreads().list(\n",
    "                    part = 'snippet',\n",
    "                    videoId = video_id,\n",
    "                      pageToken = video_response['nextPageToken']\n",
    "                ).execute()\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return pd.DataFrame(comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_ID = \"_ytf121lEbs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = video_comments(VIDEO_ID, API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through comments and predict emotions\n",
    "for index, row in comments_df.iterrows():\n",
    "    comment = row['Comment']\n",
    "    comments_df.at[index, 'Emotion'] = str(infer(comment,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mi kieu</td>\n",
       "      <td>chÃºc ah Ä‘á»™ nhiá»u sá»©c khoáº» lÃ m live Ä‘áº¿n 100 tuá»•i</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aemi</td>\n",
       "      <td>a masew nhÆ° gia cÃ¡t lÆ°á»£ng Ã­ nhá», lÃ m Ä‘Æ°á»£c beat...</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HTL QC</td>\n",
       "      <td>D2T tÆ°á»Ÿng vá» Ä‘á»™i cá»§a Big rá»“i thÃ¬ Ä‘i theo team ...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trongduc9</td>\n",
       "      <td>thÃ´i cÅ©ng muá»™n rá»“i cÃ¡c e vá» máº¹ Ä‘i nhá»ƒ</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Myanhh</td>\n",
       "      <td>tuti hiá»n Ã¡c</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Long Nguyá»…n</td>\n",
       "      <td>hÃ­ hÃ­ em xem Ä‘áº§u áº¡</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>CÃ´ng ChÃ­</td>\n",
       "      <td>Xem Ä‘áº§u haha</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Mai PhÆ°Æ¡ng</td>\n",
       "      <td>Äáº§u</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Linh xinh ngai</td>\n",
       "      <td>Hello a</td>\n",
       "      <td>Enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>upclx123</td>\n",
       "      <td>Ãc =)))</td>\n",
       "      <td>Disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author                                            Comment  \\\n",
       "0           mi kieu    chÃºc ah Ä‘á»™ nhiá»u sá»©c khoáº» lÃ m live Ä‘áº¿n 100 tuá»•i   \n",
       "1              aemi  a masew nhÆ° gia cÃ¡t lÆ°á»£ng Ã­ nhá», lÃ m Ä‘Æ°á»£c beat...   \n",
       "2            HTL QC  D2T tÆ°á»Ÿng vá» Ä‘á»™i cá»§a Big rá»“i thÃ¬ Ä‘i theo team ...   \n",
       "3         trongduc9              thÃ´i cÅ©ng muá»™n rá»“i cÃ¡c e vá» máº¹ Ä‘i nhá»ƒ   \n",
       "4            Myanhh                                       tuti hiá»n Ã¡c   \n",
       "..              ...                                                ...   \n",
       "306     Long Nguyá»…n                                 hÃ­ hÃ­ em xem Ä‘áº§u áº¡   \n",
       "307        CÃ´ng ChÃ­                                       Xem Ä‘áº§u haha   \n",
       "308      Mai PhÆ°Æ¡ng                                                Äáº§u   \n",
       "309  Linh xinh ngai                                            Hello a   \n",
       "310        upclx123                                            Ãc =)))   \n",
       "\n",
       "       Emotion  \n",
       "0    Enjoyment  \n",
       "1    Enjoyment  \n",
       "2        Other  \n",
       "3        Other  \n",
       "4    Enjoyment  \n",
       "..         ...  \n",
       "306  Enjoyment  \n",
       "307  Enjoyment  \n",
       "308      Other  \n",
       "309  Enjoyment  \n",
       "310    Disgust  \n",
       "\n",
       "[311 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.to_excel('Comments.xlsx', index=False)\n",
    "comments_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_sent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
