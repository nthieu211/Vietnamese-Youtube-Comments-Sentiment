{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nescessary library\n",
    "\n",
    "!pip install emoji\n",
    "\n",
    "!pip install underthesea\n",
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import string\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import flatten\n",
    "import underthesea\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup, AutoTokenizer, AutoModel, logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analytics and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://github.com/nguyenvanhieuvn/text-classification-tutorial/blob/master/text_classification_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML code\n",
    "def remove_HTML(text):\n",
    "    return re.sub(r'<[^>]*>', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize unicode\n",
    "def convert_unicode(text):\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "    charutf8 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "    char1252 = char1252.split('|')\n",
    "    charutf8 = charutf8.split('|')\n",
    "    \n",
    "    dic = {}\n",
    "    for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dic[x.group()], text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize accent typing\n",
    "vowels_to_ids = {}\n",
    "vowels_table = [\n",
    "    ['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a' ],\n",
    "    ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "    ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "    ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e' ],\n",
    "    ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "    ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i' ],\n",
    "    ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o' ],\n",
    "    ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "    ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "    ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u' ],\n",
    "    ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "    ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y' ]\n",
    "]\n",
    "\n",
    "for i in range(len(vowels_table)):\n",
    "    for j in range(len(vowels_table[i]) - 1):\n",
    "        vowels_to_ids[vowels_table[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def is_valid_vietnamese_word(word):\n",
    "    chars = list(word)\n",
    "    vowel_indexes = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if vowel_indexes == -1: vowel_indexes = index\n",
    "            else:\n",
    "                if index - vowel_indexes != 1: return False\n",
    "                vowel_indexes = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def standardize_word_typing(word):\n",
    "    if not is_valid_vietnamese_word(word): return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    vowel_indexes = []\n",
    "    qu_or_gi = False\n",
    "\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = vowels_table[x][0]\n",
    "\n",
    "        if not qu_or_gi or index != 1:\n",
    "            vowel_indexes.append(index)\n",
    "\n",
    "    if len(vowel_indexes) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = vowels_to_ids.get(chars[1])\n",
    "                chars[1] = vowels_table[x][dau_cau]\n",
    "            else:\n",
    "                x, y = vowels_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1: chars[2] = vowels_table[x][dau_cau]\n",
    "                else: chars[1] = vowels_table[5][dau_cau] if chars[1] == 'i' else vowels_table[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in vowel_indexes:\n",
    "        x, y = vowels_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = vowels_table[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(vowel_indexes) == 2:\n",
    "        if vowel_indexes[-1] == len(chars) - 1:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[0]]]\n",
    "            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]\n",
    "        else:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    else:\n",
    "        x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def standardize_sentence_typing(text):\n",
    "    words = text.lower().split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        if len(cw) == 3: cw[1] = standardize_word_typing(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize acronyms\n",
    "!wget https://gist.githubusercontent.com/nguyenvanhieuvn/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt\n",
    "\n",
    "replace_list = {\n",
    "    'ô kêi': 'ok', 'okie': 'ok', 'o kê': 'ok', 'okey': 'ok', 'ôkê': 'ok', 'oki': 'ok', 'oke': 'ok', 'okay': 'ok', 'okê': 'ok',\n",
    "    'tks': 'cảm ơn', 'thks': 'cảm ơn', 'thanks': 'cảm ơn', 'ths': 'cảm ơn', 'thank': 'cảm ơn',\n",
    "    'kg': 'không', 'not': 'không', 'k': 'không', 'kh': 'không', 'kô': 'không', 'hok': 'không', 'ko': 'không', 'khong': 'không', 'kp': 'không phải',\n",
    "    'he he': 'tích cực', 'hehe': 'tích cực', 'hihi': 'tích cực', 'haha': 'tích cực', 'hjhj': 'tích cực', 'thick': 'tích cực',\n",
    "    'lol': 'tiêu cực', 'cc': 'tiêu cực', 'huhu': 'tiêu cực', 'cute': 'dễ thương',\n",
    "     \n",
    "    'sz': 'cỡ', 'size': 'cỡ', \n",
    "    'wa': 'quá', 'wá': 'quá', 'qá': 'quá', \n",
    "    'đx': 'được', 'dk': 'được', 'dc': 'được', 'đk': 'được', 'đc': 'được', \n",
    "    'vs': 'với', 'j': 'gì', '“': ' ', 'time': 'thời gian', 'm': 'mình', 'mik': 'mình', 'r': 'rồi', 'bjo': 'bao giờ', 'very': 'rất',\n",
    "\n",
    "    'authentic': 'chuẩn chính hãng', 'aut': 'chuẩn chính hãng', 'auth': 'chuẩn chính hãng', 'date': 'hạn sử dụng', 'hsd': 'hạn sử dụng', \n",
    "    'store': 'cửa hàng', 'sop': 'cửa hàng', 'shopE': 'cửa hàng', 'shop': 'cửa hàng', \n",
    "    'sp': 'sản phẩm', 'product': 'sản phẩm', 'hàg': 'hàng', \n",
    "    'ship': 'giao hàng', 'delivery': 'giao hàng', 'síp': 'giao hàng', 'order': 'đặt hàng',\n",
    "\n",
    "    'gud': 'tốt', 'wel done': 'tốt', 'good': 'tốt', 'gút': 'tốt', 'tot': 'tốt', 'nice': 'tốt', 'perfect': 'rất tốt', \n",
    "    'quality': 'chất lượng', 'chất lg': 'chất lượng', 'chat': 'chất', 'excelent': 'hoàn hảo', 'bt': 'bình thường',\n",
    "    'sad': 'tệ', 'por': 'tệ', 'poor': 'tệ', 'bad': 'tệ', \n",
    "    'beautiful': 'đẹp tuyệt vời', 'dep': 'đẹp', \n",
    "    'xau': 'xấu', 'sấu': 'xấu', \n",
    "     \n",
    "    'thik': 'thích', 'iu': 'yêu', 'fake': 'giả mạo', \n",
    "    'quickly': 'nhanh', 'quick': 'nhanh', 'fast': 'nhanh',\n",
    "    'fresh': 'tươi', 'delicious': 'ngon',\n",
    "\n",
    "    'dt': 'điện thoại', 'fb': 'facebook', 'face': 'facebook', 'ks': 'khách sạn', 'nv': 'nhân viên',\n",
    "    'nt': 'nhắn tin', 'ib': 'nhắn tin', 'tl': 'trả lời', 'trl': 'trả lời', 'rep': 'trả lời',\n",
    "    'fback': 'feedback', 'fedback': 'feedback',\n",
    "    'sd': 'sử dụng', 'sài': 'xài', \n",
    "\n",
    "    '^_^': 'tích cực', ':)': 'tích cực', ':(': 'tiêu cực',\n",
    "    '❤️': 'tích cực', '👍': 'tích cực', '🎉': 'tích cực', '😀': 'tích cực', '😍': 'tích cực', '😂': 'tích cực', '🤗': 'tích cực', '😙': 'tích cực', '🙂': 'tích cực', \n",
    "    '😔': 'tiêu cực', '😓': 'tiêu cực', \n",
    "    '⭐': 'star', '*': 'star', '🌟': 'star',\n",
    "}\n",
    "\n",
    "with open('teencode.txt', encoding='utf-8') as f:\n",
    "    for pair in f.readlines():\n",
    "        key, value = pair.split('\\t')\n",
    "        replace_list[key] = value.strip()\n",
    "\n",
    "\n",
    "def normalize_acronyms(text):\n",
    "    words = []\n",
    "    for word in text.strip().split():\n",
    "        # word = word.strip(string.punctuation)\n",
    "        if word.lower() not in replace_list.keys(): words.append(word)\n",
    "        else: words.append(replace_list[word.lower()])\n",
    "    return emoji.demojize(' '.join(words)) # Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word segmentation & Tokenize\n",
    "def word_segmentation(line):\n",
    "    line = underthesea.word_tokenize(line, format=\"text\")\n",
    "    return tokenizer.encode(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary characters\n",
    "def remove_unnecessary_characters(text):\n",
    "    text = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬÉÈẺẼẸÊẾỀỂỄỆÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÍÌỈĨỊÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴĐ_]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
