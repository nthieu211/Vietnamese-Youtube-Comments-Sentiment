{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nescessary library\n",
    "\n",
    "!pip install emoji\n",
    "\n",
    "!pip install underthesea\n",
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import string\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import flatten\n",
    "import underthesea\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup, AutoTokenizer, AutoModel, logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analytics and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://github.com/nguyenvanhieuvn/text-classification-tutorial/blob/master/text_classification_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML code\n",
    "def remove_HTML(text):\n",
    "    return re.sub(r'<[^>]*>', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize unicode\n",
    "def convert_unicode(text):\n",
    "    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'\n",
    "    charutf8 = 'Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´'\n",
    "    char1252 = char1252.split('|')\n",
    "    charutf8 = charutf8.split('|')\n",
    "    \n",
    "    dic = {}\n",
    "    for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]\n",
    "    return re.sub(\n",
    "        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',\n",
    "        lambda x: dic[x.group()], text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize accent typing\n",
    "vowels_to_ids = {}\n",
    "vowels_table = [\n",
    "    ['a', 'Ã ', 'Ã¡', 'áº£', 'Ã£', 'áº¡', 'a' ],\n",
    "    ['Äƒ', 'áº±', 'áº¯', 'áº³', 'áºµ', 'áº·', 'aw'],\n",
    "    ['Ã¢', 'áº§', 'áº¥', 'áº©', 'áº«', 'áº­', 'aa'],\n",
    "    ['e', 'Ã¨', 'Ã©', 'áº»', 'áº½', 'áº¹', 'e' ],\n",
    "    ['Ãª', 'á»', 'áº¿', 'á»ƒ', 'á»…', 'á»‡', 'ee'],\n",
    "    ['i', 'Ã¬', 'Ã­', 'á»‰', 'Ä©', 'á»‹', 'i' ],\n",
    "    ['o', 'Ã²', 'Ã³', 'á»', 'Ãµ', 'á»', 'o' ],\n",
    "    ['Ã´', 'á»“', 'á»‘', 'á»•', 'á»—', 'á»™', 'oo'],\n",
    "    ['Æ¡', 'á»', 'á»›', 'á»Ÿ', 'á»¡', 'á»£', 'ow'],\n",
    "    ['u', 'Ã¹', 'Ãº', 'á»§', 'Å©', 'á»¥', 'u' ],\n",
    "    ['Æ°', 'á»«', 'á»©', 'á»­', 'á»¯', 'á»±', 'uw'],\n",
    "    ['y', 'á»³', 'Ã½', 'á»·', 'á»¹', 'á»µ', 'y' ]\n",
    "]\n",
    "\n",
    "for i in range(len(vowels_table)):\n",
    "    for j in range(len(vowels_table[i]) - 1):\n",
    "        vowels_to_ids[vowels_table[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def is_valid_vietnamese_word(word):\n",
    "    chars = list(word)\n",
    "    vowel_indexes = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if vowel_indexes == -1: vowel_indexes = index\n",
    "            else:\n",
    "                if index - vowel_indexes != 1: return False\n",
    "                vowel_indexes = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def standardize_word_typing(word):\n",
    "    if not is_valid_vietnamese_word(word): return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    vowel_indexes = []\n",
    "    qu_or_gi = False\n",
    "\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = vowels_table[x][0]\n",
    "\n",
    "        if not qu_or_gi or index != 1:\n",
    "            vowel_indexes.append(index)\n",
    "\n",
    "    if len(vowel_indexes) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = vowels_to_ids.get(chars[1])\n",
    "                chars[1] = vowels_table[x][dau_cau]\n",
    "            else:\n",
    "                x, y = vowels_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1: chars[2] = vowels_table[x][dau_cau]\n",
    "                else: chars[1] = vowels_table[5][dau_cau] if chars[1] == 'i' else vowels_table[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in vowel_indexes:\n",
    "        x, y = vowels_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # Ãª, Æ¡\n",
    "            chars[index] = vowels_table[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(vowel_indexes) == 2:\n",
    "        if vowel_indexes[-1] == len(chars) - 1:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[0]]]\n",
    "            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]\n",
    "        else:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    else:\n",
    "        x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def standardize_sentence_typing(text):\n",
    "    words = text.lower().split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        if len(cw) == 3: cw[1] = standardize_word_typing(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize acronyms\n",
    "!wget https://gist.githubusercontent.com/nguyenvanhieuvn/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt\n",
    "\n",
    "replace_list = {\n",
    "    'Ã´ kÃªi': 'ok', 'okie': 'ok', 'o kÃª': 'ok', 'okey': 'ok', 'Ã´kÃª': 'ok', 'oki': 'ok', 'oke': 'ok', 'okay': 'ok', 'okÃª': 'ok',\n",
    "    'tks': 'cáº£m Æ¡n', 'thks': 'cáº£m Æ¡n', 'thanks': 'cáº£m Æ¡n', 'ths': 'cáº£m Æ¡n', 'thank': 'cáº£m Æ¡n',\n",
    "    'kg': 'khÃ´ng', 'not': 'khÃ´ng', 'k': 'khÃ´ng', 'kh': 'khÃ´ng', 'kÃ´': 'khÃ´ng', 'hok': 'khÃ´ng', 'ko': 'khÃ´ng', 'khong': 'khÃ´ng', 'kp': 'khÃ´ng pháº£i',\n",
    "    'he he': 'tÃ­ch cá»±c', 'hehe': 'tÃ­ch cá»±c', 'hihi': 'tÃ­ch cá»±c', 'haha': 'tÃ­ch cá»±c', 'hjhj': 'tÃ­ch cá»±c', 'thick': 'tÃ­ch cá»±c',\n",
    "    'lol': 'tiÃªu cá»±c', 'cc': 'tiÃªu cá»±c', 'huhu': 'tiÃªu cá»±c', 'cute': 'dá»… thÆ°Æ¡ng',\n",
    "     \n",
    "    'sz': 'cá»¡', 'size': 'cá»¡', \n",
    "    'wa': 'quÃ¡', 'wÃ¡': 'quÃ¡', 'qÃ¡': 'quÃ¡', \n",
    "    'Ä‘x': 'Ä‘Æ°á»£c', 'dk': 'Ä‘Æ°á»£c', 'dc': 'Ä‘Æ°á»£c', 'Ä‘k': 'Ä‘Æ°á»£c', 'Ä‘c': 'Ä‘Æ°á»£c', \n",
    "    'vs': 'vá»›i', 'j': 'gÃ¬', 'â€œ': ' ', 'time': 'thá»i gian', 'm': 'mÃ¬nh', 'mik': 'mÃ¬nh', 'r': 'rá»“i', 'bjo': 'bao giá»', 'very': 'ráº¥t',\n",
    "\n",
    "    'authentic': 'chuáº©n chÃ­nh hÃ£ng', 'aut': 'chuáº©n chÃ­nh hÃ£ng', 'auth': 'chuáº©n chÃ­nh hÃ£ng', 'date': 'háº¡n sá»­ dá»¥ng', 'hsd': 'háº¡n sá»­ dá»¥ng', \n",
    "    'store': 'cá»­a hÃ ng', 'sop': 'cá»­a hÃ ng', 'shopE': 'cá»­a hÃ ng', 'shop': 'cá»­a hÃ ng', \n",
    "    'sp': 'sáº£n pháº©m', 'product': 'sáº£n pháº©m', 'hÃ g': 'hÃ ng', \n",
    "    'ship': 'giao hÃ ng', 'delivery': 'giao hÃ ng', 'sÃ­p': 'giao hÃ ng', 'order': 'Ä‘áº·t hÃ ng',\n",
    "\n",
    "    'gud': 'tá»‘t', 'wel done': 'tá»‘t', 'good': 'tá»‘t', 'gÃºt': 'tá»‘t', 'tot': 'tá»‘t', 'nice': 'tá»‘t', 'perfect': 'ráº¥t tá»‘t', \n",
    "    'quality': 'cháº¥t lÆ°á»£ng', 'cháº¥t lg': 'cháº¥t lÆ°á»£ng', 'chat': 'cháº¥t', 'excelent': 'hoÃ n háº£o', 'bt': 'bÃ¬nh thÆ°á»ng',\n",
    "    'sad': 'tá»‡', 'por': 'tá»‡', 'poor': 'tá»‡', 'bad': 'tá»‡', \n",
    "    'beautiful': 'Ä‘áº¹p tuyá»‡t vá»i', 'dep': 'Ä‘áº¹p', \n",
    "    'xau': 'xáº¥u', 'sáº¥u': 'xáº¥u', \n",
    "     \n",
    "    'thik': 'thÃ­ch', 'iu': 'yÃªu', 'fake': 'giáº£ máº¡o', \n",
    "    'quickly': 'nhanh', 'quick': 'nhanh', 'fast': 'nhanh',\n",
    "    'fresh': 'tÆ°Æ¡i', 'delicious': 'ngon',\n",
    "\n",
    "    'dt': 'Ä‘iá»‡n thoáº¡i', 'fb': 'facebook', 'face': 'facebook', 'ks': 'khÃ¡ch sáº¡n', 'nv': 'nhÃ¢n viÃªn',\n",
    "    'nt': 'nháº¯n tin', 'ib': 'nháº¯n tin', 'tl': 'tráº£ lá»i', 'trl': 'tráº£ lá»i', 'rep': 'tráº£ lá»i',\n",
    "    'fback': 'feedback', 'fedback': 'feedback',\n",
    "    'sd': 'sá»­ dá»¥ng', 'sÃ i': 'xÃ i', \n",
    "\n",
    "    '^_^': 'tÃ­ch cá»±c', ':)': 'tÃ­ch cá»±c', ':(': 'tiÃªu cá»±c',\n",
    "    'â¤ï¸': 'tÃ­ch cá»±c', 'ğŸ‘': 'tÃ­ch cá»±c', 'ğŸ‰': 'tÃ­ch cá»±c', 'ğŸ˜€': 'tÃ­ch cá»±c', 'ğŸ˜': 'tÃ­ch cá»±c', 'ğŸ˜‚': 'tÃ­ch cá»±c', 'ğŸ¤—': 'tÃ­ch cá»±c', 'ğŸ˜™': 'tÃ­ch cá»±c', 'ğŸ™‚': 'tÃ­ch cá»±c', \n",
    "    'ğŸ˜”': 'tiÃªu cá»±c', 'ğŸ˜“': 'tiÃªu cá»±c', \n",
    "    'â­': 'star', '*': 'star', 'ğŸŒŸ': 'star',\n",
    "}\n",
    "\n",
    "with open('teencode.txt', encoding='utf-8') as f:\n",
    "    for pair in f.readlines():\n",
    "        key, value = pair.split('\\t')\n",
    "        replace_list[key] = value.strip()\n",
    "\n",
    "\n",
    "def normalize_acronyms(text):\n",
    "    words = []\n",
    "    for word in text.strip().split():\n",
    "        # word = word.strip(string.punctuation)\n",
    "        if word.lower() not in replace_list.keys(): words.append(word)\n",
    "        else: words.append(replace_list[word.lower()])\n",
    "    return emoji.demojize(' '.join(words)) # Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word segmentation & Tokenize\n",
    "def word_segmentation(line):\n",
    "    line = underthesea.word_tokenize(line, format=\"text\")\n",
    "    return tokenizer.encode(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary characters\n",
    "def remove_unnecessary_characters(text):\n",
    "    text = re.sub(r'[^\\s\\wÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘ÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†Ã“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢ÃÃŒá»ˆÄ¨á»ŠÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä_]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
