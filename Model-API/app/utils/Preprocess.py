import regex as re
import emoji
import py_vncorenlp
import os

# Start VnCoreNLP
vncorenlp_path = os.path.join(os.path.dirname(__file__), "vncorenlp")
if not os.path.exists(os.path.join(vncorenlp_path, "VnCoreNLP-1.2.jar")):
    raise FileNotFoundError(f"Not found VnCoreNLP at {vncorenlp_path}")
rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=["wseg"], save_dir=vncorenlp_path)


# Remove HTML code
def remove_HTML_URL(text):
    html = re.compile(r"<[^>]+>")
    text = html.sub("", text).replace("\r", " ")
    text = re.sub(r"\S*(https|http)?:\S*", "", text)  # Remove URL
    return text


# Standardize unicode
def convert_unicode(text):
    char1252 = "aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£"
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´"
    char1252 = char1252.split("|")
    charutf8 = charutf8.split("|")

    dic = {}
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return re.sub(
        r"aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£",
        lambda x: dic[x.group()],
        text,
    )


# Standardize accent typing
vowels_to_ids = {}
vowels_table = [
    ["a", "Ã ", "Ã¡", "áº£", "Ã£", "áº¡", "a"],
    ["Äƒ", "áº±", "áº¯", "áº³", "áºµ", "áº·", "aw"],
    ["Ã¢", "áº§", "áº¥", "áº©", "áº«", "áº­", "aa"],
    ["e", "Ã¨", "Ã©", "áº»", "áº½", "áº¹", "e"],
    ["Ãª", "á»", "áº¿", "á»ƒ", "á»…", "á»‡", "ee"],
    ["i", "Ã¬", "Ã­", "á»‰", "Ä©", "á»‹", "i"],
    ["o", "Ã²", "Ã³", "á»", "Ãµ", "á»", "o"],
    ["Ã´", "á»“", "á»‘", "á»•", "á»—", "á»™", "oo"],
    ["Æ¡", "á»", "á»›", "á»Ÿ", "á»¡", "á»£", "ow"],
    ["u", "Ã¹", "Ãº", "á»§", "Å©", "á»¥", "u"],
    ["Æ°", "á»«", "á»©", "á»­", "á»¯", "á»±", "uw"],
    ["y", "á»³", "Ã½", "á»·", "á»¹", "á»µ", "y"],
]

for i in range(len(vowels_table)):
    for j in range(len(vowels_table[i]) - 1):
        vowels_to_ids[vowels_table[i][j]] = (i, j)


def is_valid_vietnamese_word(word):
    chars = list(word)
    vowel_indexes = -1
    for index, char in enumerate(chars):
        x, y = vowels_to_ids.get(char, (-1, -1))
        if x != -1:
            if vowel_indexes == -1:
                vowel_indexes = index
            else:
                if index - vowel_indexes != 1:
                    return False
                vowel_indexes = index
    return True


def standardize_word_typing(word):
    if not is_valid_vietnamese_word(word):
        return word
    chars = list(word)
    dau_cau = 0
    vowel_indexes = []
    qu_or_gi = False

    for index, char in enumerate(chars):
        x, y = vowels_to_ids.get(char, (-1, -1))
        if x == -1:
            continue
        elif x == 9:  # check qu
            if index != 0 and chars[index - 1] == "q":
                chars[index] = "u"
                qu_or_gi = True
        elif x == 5:  # check gi
            if index != 0 and chars[index - 1] == "g":
                chars[index] = "i"
                qu_or_gi = True

        if y != 0:
            dau_cau = y
            chars[index] = vowels_table[x][0]

        if not qu_or_gi or index != 1:
            vowel_indexes.append(index)

    if len(vowel_indexes) < 2:
        if qu_or_gi:
            if len(chars) == 2:
                x, y = vowels_to_ids.get(chars[1])
                chars[1] = vowels_table[x][dau_cau]
            else:
                x, y = vowels_to_ids.get(chars[2], (-1, -1))
                if x != -1:
                    chars[2] = vowels_table[x][dau_cau]
                else:
                    chars[1] = (
                        vowels_table[5][dau_cau]
                        if chars[1] == "i"
                        else vowels_table[9][dau_cau]
                    )
            return "".join(chars)
        return word

    for index in vowel_indexes:
        x, y = vowels_to_ids[chars[index]]
        if x == 4 or x == 8:  # Ãª, Æ¡
            chars[index] = vowels_table[x][dau_cau]
            return "".join(chars)

    if len(vowel_indexes) == 2:
        if vowel_indexes[-1] == len(chars) - 1:
            x, y = vowels_to_ids[chars[vowel_indexes[0]]]
            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]
        else:
            x, y = vowels_to_ids[chars[vowel_indexes[1]]]
            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]
    else:
        x, y = vowels_to_ids[chars[vowel_indexes[1]]]
        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]
    return "".join(chars)


def standardize_sentence_typing(text):
    words = text.lower().split()
    for index, word in enumerate(words):
        cw = re.sub(r"(^\p{P}*)([p{L}.]*\p{L}+)(\p{P}*$)", r"\1/\2/\3", word).split("/")
        if len(cw) == 3:
            cw[1] = standardize_word_typing(cw[1])
        words[index] = "".join(cw)
    return " ".join(words)


# Normalize acronyms
# !wget https://gist.githubusercontent.com/nguyenvanhieuvn/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt

replace_list = {
    "Ã´ kÃªi": "ok",
    "okie": "ok",
    "o kÃª": "ok",
    "okey": "ok",
    "Ã´kÃª": "ok",
    "Ã´ kÃª": "ok",
    "oki": "ok",
    "oke": "ok",
    "okay": "ok",
    "okÃª": "ok",
    "tks": "cáº£m Æ¡n",
    "thks": "cáº£m Æ¡n",
    "thanks": "cáº£m Æ¡n",
    "ths": "cáº£m Æ¡n",
    "thank": "cáº£m Æ¡n",
    "kg": "khÃ´ng",
    "not": "khÃ´ng",
    "k": "khÃ´ng",
    "kh": "khÃ´ng",
    "kÃ´": "khÃ´ng",
    "hok": "khÃ´ng",
    "ko": "khÃ´ng",
    "khong": "khÃ´ng",
    "kp": "khÃ´ng pháº£i",
    "he he": "cÆ°á»i",
    "hehe": "cÆ°á»i",
    "hihi": "cÆ°á»i",
    "haha": "cÆ°á»i",
    "hjhj": "cÆ°á»i",
    "thick": "thÃ­ch",
    "huhu": "khÃ³c",
    "cute": "dá»… thÆ°Æ¡ng",
    "cc": "chá»­i tá»¥c",
    "cáº·c": "chá»­i tá»¥c",
    "dm": "chá»­i tá»¥c",
    "dmm": "chá»­i tá»¥c",
    "dume": "chá»­i tá»¥c",
    "dma": "chá»­i tá»¥c",
    "vl": "chá»­i tá»¥c",
    "sz": "cá»¡",
    "size": "cá»¡",
    "wa": "quÃ¡",
    "wÃ¡": "quÃ¡",
    "qÃ¡": "quÃ¡",
    "Ä‘x": "Ä‘Æ°á»£c",
    "dk": "Ä‘Æ°á»£c",
    "dc": "Ä‘Æ°á»£c",
    "Ä‘k": "Ä‘Æ°á»£c",
    "Ä‘c": "Ä‘Æ°á»£c",
    "vs": "vá»›i",
    "j": "gÃ¬",
    "â€œ": " ",
    "time": "thá»i gian",
    "m": "mÃ¬nh",
    "mik": "mÃ¬nh",
    "r": "rá»“i",
    "bjo": "bao giá»",
    "very": "ráº¥t",
    "nk": "nÃ³",
    "authentic": "chuáº©n chÃ­nh hÃ£ng",
    "aut": "chuáº©n chÃ­nh hÃ£ng",
    "auth": "chuáº©n chÃ­nh hÃ£ng",
    "date": "háº¡n sá»­ dá»¥ng",
    "hsd": "háº¡n sá»­ dá»¥ng",
    "store": "cá»­a hÃ ng",
    "sop": "cá»­a hÃ ng",
    "sá»‘p": "cá»­a hÃ ng",
    "shop": "cá»­a hÃ ng",
    "sp": "sáº£n pháº©m",
    "product": "sáº£n pháº©m",
    "hÃ g": "hÃ ng",
    "sgk": "sÃ¡ch giÃ¡o khoa",
    "cv": "cÃ´ng viá»‡c",
    "ship": "giao hÃ ng",
    "delivery": "giao hÃ ng",
    "sÃ­p": "giao hÃ ng",
    "order": "Ä‘áº·t hÃ ng",
    "gud": "tá»‘t",
    "wel done": "tá»‘t",
    "good": "tá»‘t",
    "gÃºt": "tá»‘t",
    "tot": "tá»‘t",
    "nice": "tá»‘t",
    "perfect": "ráº¥t tá»‘t",
    "quality": "cháº¥t lÆ°á»£ng",
    "cháº¥t lg": "cháº¥t lÆ°á»£ng",
    "excelent": "hoÃ n háº£o",
    "bt": "bÃ¬nh thÆ°á»ng",
    "bth": "bÃ¬nh thÆ°á»ng",
    "sad": "tá»‡",
    "por": "tá»‡",
    "poor": "tá»‡",
    "bad": "tá»‡",
    "beautiful": "Ä‘áº¹p tuyá»‡t vá»i",
    "dep": "Ä‘áº¹p",
    "xau": "xáº¥u",
    "sáº¥u": "xáº¥u",
    "thik": "thÃ­ch",
    "iu": "yÃªu",
    "fake": "giáº£ máº¡o",
    "quickly": "nhanh",
    "quick": "nhanh",
    "fast": "nhanh",
    "fresh": "tÆ°Æ¡i",
    "delicious": "ngon",
    "dt": "Ä‘iá»‡n thoáº¡i",
    "fb": "facebook",
    "face": "facebook",
    "ks": "khÃ¡ch sáº¡n",
    "nv": "nhÃ¢n viÃªn",
    "nt": "nháº¯n tin",
    "ib": "nháº¯n tin",
    "tl": "tráº£ lá»i",
    "trl": "tráº£ lá»i",
    "rep": "tráº£ lá»i",
    "fback": "feedback",
    "fedback": "feedback",
    "sd": "sá»­ dá»¥ng",
    "sÃ i": "xÃ i",
    "^_^": "cÆ°á»i",
    ":)": "cÆ°á»i",
    ":))": "cÆ°á»i",
    ":)))": "cÆ°á»i",
    "=))": "cÆ°á»i",
    "=)))": "cÆ°á»i",
    ":(": "buá»“n",
    ":((": "buá»“n",
    ":(((": "buá»“n",
    "â¤ï¸": "yÃªu thÃ­ch",
    "ğŸ‘": "thÃ­ch",
    "ğŸ‰": "chÃºc má»«ng",
    "ğŸ˜€": "cÆ°á»i",
    "ğŸ˜": "yÃªu thÃ­ch",
    "ğŸ˜‚": "cÆ°á»i lá»›n",
    "ğŸ¤£": "cÆ°á»i lá»›n",
    "ğŸ¤—": "vá»— tay",
    "ğŸ˜™": "cÆ°á»i",
    "ğŸ™‚": "cÆ°á»i",
    "ğŸ˜”": "buá»“n",
    "ğŸ˜“": "buá»“n",
    "T_T": "khÃ³c",
    "ğŸ˜­": "khÃ³c lá»›n",
    "ğŸ˜¡": "giáº­n dá»¯",
    "ğŸ¤¬": "giáº­n dá»¯",
    "ğŸ¤¡": "máº·t chÃº há»",
    "ğŸ˜©": "tháº¥t vá»ng",
    "ğŸ˜": "tháº¥t vá»ng",
    "ğŸ˜¢": "xÃºc Ä‘á»™ng",
    "â­": "star",
    "*": "star",
    "ğŸŒŸ": "star",
}

with open(
    os.path.join(os.path.dirname(__file__), "teencode.txt"), encoding="utf-8"
) as f:
    for pair in f.readlines():
        key, value = pair.split("\t")
        replace_list[key] = value.strip()


def normalize_acronyms(text):
    words = []
    for word in text.strip().split():
        if word.lower() not in replace_list.keys():
            words.append(word)
        else:
            words.append(replace_list[word.lower()])
    return emoji.demojize(" ".join(words))  # Remove Emojis


# Word segmentation & Tokenize
def word_segmentation(text):
    return rdrsegmenter.word_segment(text)


def text_preprocess(text):
    text = convert_unicode(text)
    text = standardize_sentence_typing(text)
    text = normalize_acronyms(text)
    text = re.sub(
        r"[^\s\wÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘_]",
        " ",
        text,
    )  # Remove unnecessary character
    text = re.sub(r"\s+", " ", text).strip()  # Remove spacing > 1
    text = word_segmentation(text)  # required for PhoBERT
    text = " ".join(text)  # return 1 string
    return text
